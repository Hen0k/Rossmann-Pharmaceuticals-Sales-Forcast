{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling with Regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from src.fetch_data import DataLoader\n",
    "from src.cleaning import CleanDataFrame\n",
    "from src.visualization import Plotters\n",
    "from src.processing import FeatureEngineering\n",
    "from src.modeling_pipeline import (\n",
    "    run_train_pipeline, \n",
    "    get_pipeline,\n",
    "    )\n",
    "\n",
    "\n",
    "import mlflow\n",
    "\n",
    "\n",
    "cleaner = CleanDataFrame()\n",
    "feature_engineering = FeatureEngineering()\n",
    "plotters = Plotters(w=6, h=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hat/dev-env/10Acadamy/week_3/Rossmann-Pharmaceuticals-Sales-Forcast/notebooks/../src/fetch_data.py:28: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(io.StringIO(content), sep=\",\")\n",
      "DataLoaderLogger - INFO - DVC: CSV file read with path: data/merged/train.csv | version: merged_v3 | from: ../\n"
     ]
    }
   ],
   "source": [
    "# Then load the raw sales data\n",
    "data_path = 'data/merged/train.csv'\n",
    "version = 'merged_v3'\n",
    "repo = '../'\n",
    "\n",
    "train_df = DataLoader.dvc_get_data(data_path, version, repo)\n",
    "train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
    "\n",
    "# load the test data\n",
    "# data_path = 'data/merged/test.csv'\n",
    "# version = 'merged_v3'\n",
    "# repo = '../'\n",
    "\n",
    "# test_df = DataLoader.dvc_get_data(data_path, version, repo)\n",
    "# test_df['Date'] = pd.to_datetime(test_df['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will add the additional columns from the feature engineering here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FeatureEngineeringLogger - INFO - 9 new columns added to the dataframe\n",
      "FeatureEngineeringLogger - INFO - Feature enginerring completed\n"
     ]
    }
   ],
   "source": [
    "train_df = feature_engineering.transform(train_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to properly split the data, we need the timeframe for it. I will grab all the unique dates in the `Date` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts at 2013-01-01T00:00:00.000000000 --- ends at: 2015-07-31T00:00:00.000000000\n",
      "It spans for 942 days\n"
     ]
    }
   ],
   "source": [
    "timeframe = train_df.Date.unique()\n",
    "timeframe.sort()\n",
    "print(f\"Starts at {timeframe[0]} --- ends at: {timeframe[-1]}\")\n",
    "print(f\"It spans for {len(timeframe)} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.sort_values(by='Date', inplace=True)\n",
    "\n",
    "train_split, test_split = train_df.iloc[:int(len(train_df)*.8), :], train_df.iloc[int(len(train_df)*.8):, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "n_future = 1\n",
    "n_past = 32\n",
    "train_split = train_split.iloc[:20_000, :]\n",
    "test_split = test_split.iloc[:20_000, :]\n",
    "for i in range(n_past, len(train_split) - n_future +1):\n",
    "    X_train.append(train_split.iloc[i - n_past:i, :])\n",
    "    y_train.append(train_split.iloc[i:i + n_future, 0])\n",
    "\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "for i in range(n_past, len(test_split) - n_future +1):\n",
    "    X_test.append(test_split.iloc[i - n_past:i, :])\n",
    "    y_test.append(test_split.iloc[i:i + n_future, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train data shape: (19968, 32, 29)\n",
      "y_train data shape: (19968, 1)\n",
      "X_test data shape: (19968, 32, 29)\n",
      "y_test data shape: (19968, 1)\n"
     ]
    }
   ],
   "source": [
    "# Let's turn them into numpy arrays\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "print(\"X_train data shape:\", X_train.shape)\n",
    "print(\"y_train data shape:\", y_train.shape)\n",
    "\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "print(\"X_test data shape:\", X_test.shape)\n",
    "print(\"y_test data shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Embedding\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Initializing the LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # Adding 1st LSTM layer\n",
    "    model.add(LSTM(units=32, return_sequences=True, input_shape=(X_train.shape[1:])))\n",
    "    model.add(Dense(units=64, activation='relu'))\n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=128, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=256, return_sequences=True))\n",
    "    #model.add(LSTM(units=1024, return_sequences=True))\n",
    "    #model.add(Dense(units=512, activation='relu'))\n",
    "\n",
    "    # Adding 2nd LSTM layer\n",
    "    model.add(LSTM(units=128, return_sequences=True))\n",
    "\n",
    "    model.add(LSTM(units=32, return_sequences=False))\n",
    "\n",
    "    # Adding Dropout\n",
    "    model.add(Dropout(0.2))\n",
    "    #model.add(Dense(units=16, activation='relu'))\n",
    "    # Output layer\n",
    "    model.add(Dense(units=y_train.shape[1]))\n",
    "    # # Compiling the Neural Network\n",
    "    model.compile(optimizer = Adam(learning_rate=0.01), loss='mse')\n",
    "\n",
    "    return model\n",
    "\n",
    "# tb = TensorBoard('logs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Train with cross-validation\n",
    "\n",
    "the `n_splits` parameter can be modified to increase of decrease the number of folds to train with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "k_fold = KFold(n_splits=5, shuffle=False)\n",
    "fold = 1\n",
    "train_histories = []\n",
    "loss_per_fold = []\n",
    "for train, valid in k_fold.split(X_train, y_train):\n",
    "    # Get a fress model from the factory :)\n",
    "    model = get_model()\n",
    "    # Training utilities\n",
    "    es = EarlyStopping(monitor='val_loss', min_delta=1e-10, patience=10, verbose=0)\n",
    "    rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0)\n",
    "    mcp = ModelCheckpoint(filepath=f'weights-fold-{fold}.h5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True)\n",
    "    # Start training\n",
    "    history = model.fit(X_train, \n",
    "                    y_train, \n",
    "                    shuffle=True, \n",
    "                    epochs=100, \n",
    "                    callbacks=[es, rlr, mcp], \n",
    "                    validation_split=0.2, \n",
    "                    verbose=0, \n",
    "                    batch_size=64)\n",
    "    train_histories.append(history)\n",
    "    print(f\"Fold number {fold} complete\")\n",
    "    print(\"-\"*100)\n",
    "    # Evaluate\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    loss_per_fold.append(score)\n",
    "    print(f'Score for fold {fold}: {score}')\n",
    "    fold += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Loads the best weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fold = loss_per_fold.index(min(loss_per_fold)) + 1\n",
    "print(\"The best fold is: \", best_fold)\n",
    "checkpoint_path = f\"weights-fold-{best_fold}.h5\"\n",
    "model.load_weights(checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ae17cd3bd788a94320e0d05b6452017a9651648b673e7b352c316aae5e41a4d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
